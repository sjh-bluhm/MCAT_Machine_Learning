{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e1a9c-203d-47f3-9374-06c0a820addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "#Python >= 3.5\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "#Scikit-Learn >= 0.20\n",
    "import sklearn as sk\n",
    "assert sk.__version__ >= \"0.20\"\n",
    "\n",
    "#numpy for calculations\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#matplotlib for figures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#a magic function that allows inline plotting so figures are rendered in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#pandas for ...\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#add other needed modules here\n",
    "#Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e73ee9-5e9e-4554-bc21-ea8bffbbc7d9",
   "metadata": {},
   "source": [
    "## Setup and Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf59d5-b8c5-442e-950b-2a3a2c157499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from the cv file using pandas\n",
    "scores = pd.read_csv(\"Complete MCAT Data.csv\", header = 1)\n",
    "\n",
    "# rename some columns for consistency\n",
    "scores.rename(columns = {'Days Before Exam':'Days Before Exam.1', 'Days Before Exam.1':'Days Before Exam.2', 'Days Before Exam.2':'Days Before Exam.3', 'Days Before Exam.3':'Days Before Exam.4'}, inplace = True)\n",
    "\n",
    "# preview first five lines\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d2f37-ec9d-4fad-94db-012c6dbdd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "scores.drop(columns=['Timestamp', 'Exam Date', 'C/P Score', 'CARS Score', 'B/B Score', 'P/S Score', 'Total Score', 'Total Score.1', 'Total Score.2', 'Total Score.3'], inplace = True)\n",
    "\n",
    "# sanity check: take a look at the data\n",
    "print('Scores data shape:', scores.shape)\n",
    "\n",
    "# look at distribution of scores\n",
    "scores['Real Score'].value_counts()\n",
    "\n",
    "# gives count, mean, standard deviation, min, max, and percentiles (including median)\n",
    "scores.describe(percentiles = [0.25, 0.5, 0.75], include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aa63d-22f2-4676-89c7-8df143b08b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data statistics for the report\n",
    "# for report: (state the number of datapoints, briefly describe the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42831d28-12cf-4493-b7d9-e9e0ab74b3a4",
   "metadata": {},
   "source": [
    "## Visualize and Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368028e-694d-41d8-bd58-d46b8816a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# look at how many values are missing in each dataframe column\n",
    "missing_values_table(scores)\n",
    "\n",
    "# visualize missing data with Missingno\n",
    "msno.bar(scores)\n",
    "msno.matrix(scores)\n",
    "\n",
    "# see if there is a reason for missing data\n",
    "msno.heatmap(scores)\n",
    "msno.dendrogram(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fba8c-b343-4efc-aeed-29c002f997a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear regression, drop all rows with missing values\n",
    "scores.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# convert all remaining data to integers\n",
    "for col in scores.columns:\n",
    "        scores[col] = scores[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369dde2-abce-43c9-8617-c763143e58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of columns, column labels, column data types, memory usage, range index, and non-null number of cells in each column\n",
    "scores.info()\n",
    "\n",
    "# preview data\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e58c9-2563-40fc-87d9-9a6f446ace5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for isnegative in (scores > 0).all(1)[i]:\n",
    "#        if isnegative..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41faa40-0247-4146-b3cb-b65297f108e8",
   "metadata": {},
   "source": [
    "## Clean Data and Add New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afd70c-e0fd-4f88-a3ea-2fe8a9ddb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with # negative values\n",
    "#                # days before exam > 180\n",
    "#                # exact duplicate values to another row\n",
    "#                # impossible scores (>528 total or >132 on any subsection)\n",
    "remove_indices = []\n",
    "remove_indices.extend(scores[scores['Real Score']>528].index.values)\n",
    "remove_indices.extend(scores[scores['Real Score']<472].index.values)\n",
    "for i in range(1, 5):\n",
    "    remove_indices.extend(scores[scores['Days Before Exam.'+str(i)]>200].index.values)\n",
    "    remove_indices.extend(scores[scores['C/P Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['CARS Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['B/B Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['P/S Score.'+str(i)]>132].index.values)\n",
    "\n",
    "# remove duplicate indices\n",
    "remove_indices = list(dict.fromkeys(remove_indices))\n",
    "\n",
    "# delete all rows with impossible values\n",
    "for i in remove_indices:\n",
    "    scores.drop(i, axis = 0, inplace = True)\n",
    "\n",
    "scores.info()\n",
    "scores.head()\n",
    "\n",
    "# add custom attribute: variance between test scores (if multiple test scores)\n",
    "# NOTE: make sure variance is calculated between different tests, not between subsections of the same test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fcaa9-3296-4034-a4f8-ac68ef8c506f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa30b3f4-015e-409d-a994-e98324158585",
   "metadata": {},
   "source": [
    "## Create a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b23205a-8aa0-46e7-b0ee-0eca9c83688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and labels from the observations\n",
    "features = scores.columns[1:len(scores)]\n",
    "X = scores[features].values.reshape(-1, len(features))\n",
    "y = scores[\"Real Score\"].to_numpy()\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0ebbe-e6cf-4cca-a9fe-3eb094a38c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into training and testing, fix random_state so output is the same every run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True,\n",
    "                                   test_size=0.20, random_state=0)\n",
    "\n",
    "#sanity check: look at how many data points are in train/test\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13b459-b7e4-4d80-b906-a28455a1334e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check the distributions of the training and testing sets\n",
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03acd4-a750-42eb-9536-b53d14ba441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734aa983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sanity check: preview test and training data\n",
    "print(X_train)\n",
    "print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd3985-d8ec-46e6-986b-2ccd58ea7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#perform stratified shufflesplit cross-validator\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "#compares random and stratified error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d45b6b-5fc9-4a90-a653-6f533416cb21",
   "metadata": {},
   "source": [
    "## Visualize Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213876c-f1d2-48d3-a7f9-afbec1c4fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix with pandas\n",
    "corr_matrix = scores.corr()\n",
    "corr_matrix['Real Score'].sort_values(ascending=False)\n",
    "\n",
    "# create scatter matrices displaying totals, medians.\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = ['Real Score', 'CARS Score.1', 'CARS Score.2', 'CARS Score.3', 'CARS Score.4']\n",
    "scatter_matrix(scores[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699553e4-7824-45ca-8d7d-9cd13f98c871",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08bcb4-b8e9-447f-9fbe-248597d3b5b0",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff402a-5bb9-4fb7-85c5-59c8be722a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lin_reg0 = LinearRegression()\n",
    "lin_reg0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ba533-9e64-430e-b8ef-670b66d9f619",
   "metadata": {},
   "source": [
    "## Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d94da-6cf3-4ea5-8ff0-db33a1e2925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5279635-3517-4491-954d-b15561871310",
   "metadata": {},
   "source": [
    "## Loss Function and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2b6c1-e839-4413-be8f-35a7e46b2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def lin_error(y_val, y_pred):\n",
    "    import math\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    assert len(y_pred) == len(y_val), \"the length of y_pred is incorrect\"\n",
    "    lin_mse = mean_squared_error(y_val, y_pred, squared=True)\n",
    "    lin_rmse = math.sqrt(lin_mse)\n",
    "    print(\"mean squared error: \", lin_mse)\n",
    "    print(\"root mean squared error: \", lin_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583b335-7874-440b-95f3-2038acf09882",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin_reg0.predict(X_val)\n",
    "\n",
    "# compute accuracy on the training set\n",
    "accuracy = lin_reg0.score(X_val, y_val)\n",
    "\n",
    "print(\"accuracy of LinReg : \", accuracy)\n",
    "\n",
    "assert len(y_pred) == len(y_val), \"the length of y_pred is incorrect\"\n",
    "\n",
    "lin_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b272a6c-1d01-4ba8-9d5a-8a6c78b4c0e9",
   "metadata": {},
   "source": [
    "# Linear Regression with Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0286b-13b6-4921-a85b-a444d2d9d2c7",
   "metadata": {},
   "source": [
    "## PCA to Create Linear Combinations of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b093fc-1046-43e7-882d-6782c98595b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# This code is from Assignment 2\n",
    "\n",
    "# fit the PCA\n",
    "N = 8\n",
    "pca = PCA(n_components=N)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_val_reduced = pca.fit_transform(X_val)\n",
    "\n",
    "# plot the explained variances\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "color = 'tab:blue'\n",
    "ax1.bar(1+np.arange(N), pca.explained_variance_ratio_, color=color)\n",
    "ax1.set_xticks(1+np.arange(N, step=2))\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel(\"Explained variance ratio\", color=color)\n",
    "ax1.set_xlabel(\"Generated feature\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.plot(1+np.arange(N), np.cumsum(pca.explained_variance_ratio_), color=color)\n",
    "ax2.set_ylabel(\"Cumulative explained variance ratio\", color=color)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9dde2b-ecb0-4188-aece-a5f3d5133630",
   "metadata": {},
   "source": [
    "## Training and Evaluating on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e0a86f-9ca3-4a6b-91e1-a1e4d85a1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a linear regression object\n",
    "lin_reg1 = LinearRegression() #solver = ?\n",
    "lin_reg1.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d82488-258d-4b26-b76b-b8b6d042d93a",
   "metadata": {},
   "source": [
    "## Validate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9b309-dad5-4edd-899d-b711a2c61e20",
   "metadata": {},
   "source": [
    "## Loss Function and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22003fb7-b472-48ea-a212-43d1f5b4a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_accuracy = lin_reg.score(X_val_reduced, y_val)\n",
    "print(f\"Prediction accuracy: {100*lin_accuracy:.2f}%\")\n",
    "\n",
    "y_pred = lin_reg.predict(X_val_reduced)\n",
    "\n",
    "# loss function\n",
    "lin_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971bf4d-76e4-4070-92b6-ba2c734d1847",
   "metadata": {},
   "source": [
    "# Compare Linear Regression with and without Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400aabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a linear regression object\n",
    "lin_reg = LinearRegression() #solver = ?\n",
    "lin_reg.fit(X_train_reduced, y_train)\n",
    "\n",
    "lin_accuracy = lin_reg.score(X_val_reduced, y_val)\n",
    "\n",
    "# sanity check: test out predictions and compare to labels\n",
    "#print(lin_reg.predict(X_train_reduced))\n",
    "#print(list(y_train))\n",
    "\n",
    "print(f\"Prediction accuracy: {100*lin_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = lin_reg.predict(X_val_reduced)\n",
    "\n",
    "assert len(y_pred) == len(y_val), \"the length of y_pred is incorrect\"\n",
    "\n",
    "lin_rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1af4ce-85a6-479a-972f-8dc59f16dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, ###, ###,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", lin_scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3879f73-9e88-4808-88ec-c298f1468d9f",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c02cd-1fa0-4509-b9d2-71c7efe63f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display what pca 1 is\n",
    "plt.figure()\n",
    "\n",
    "print(X.shape)\n",
    "D,E = np.linalg.eig(np.matmul(X.T,X))\n",
    "#print(D)\n",
    "#print(D.shape)\n",
    "#E = E.reshape()\n",
    "z = list(scores.columns[1:])#np.arange(1,21)\n",
    "#print(z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (8,4))\n",
    "# We see that the highest eigenvalue is D[7] so row 7 of E is \n",
    "ax.bar(z, E[:,7])#, label=bar_labels, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel('PCA 1 dimension')\n",
    "ax.set_title('PCA 1 in fonction of the features')\n",
    "for label in ax.get_xticklabels(which='major'):\n",
    "    label.set(rotation=30, horizontalalignment='right',fontsize=8)\n",
    "#ax.legend(title='Fruit color')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.plot(z,E[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a7f6b-4a5c-4ac1-a77a-70d9a2bc58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "# convert the labels to numbers, each will be assigned a separate color based on the cmap specified\n",
    "colors = [int(x) for x in y_train]\n",
    "sc = plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], c=colors)#,s=1 cmap='tab10')\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend(*sc.legend_elements(), title='digit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd1a60-c2b0-4d7f-b4a0-d857b31ce343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plotting\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], X_train_reduced[:,2], c=colors)\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "plt.legend(*sc.legend_elements(), title='digit',loc=\"upper center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664e31f-b89e-4f07-91db-c94e4f9bb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plotting\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], y_train)#, c=colors)\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel(\"Final score\")\n",
    "#plt.legend(*sc.legend_elements(), title='digit',loc=\"upper center\")\n",
    "plt.title(\"Final scores at exam of the training set in function of PCA1 and PCA2 parameters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7d7dd-14ad-4ef5-bbbb-6beba94414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "# convert the labels to numbers, each will be assigned a separate color based on the cmap specified\n",
    "colors = [int(x) for x in y_train]\n",
    "sc = plt.scatter(X_train_reduced[:, 0],y_train)#, c=colors)#,s=1 cmap='tab10')\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"Final score\")\n",
    "#plt.legend(*sc.legend_elements(), title='digit')\n",
    "plt.title(\"Final scores at exam of the training set in function of PCA1 parameter\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
