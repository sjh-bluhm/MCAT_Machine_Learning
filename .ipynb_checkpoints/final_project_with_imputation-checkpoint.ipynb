{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "#Python >= 3.5\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "#Scikit-Learn >= 0.20\n",
    "import sklearn as sk\n",
    "assert sk.__version__ >= \"0.20\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#numpy for calculations\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#matplotlib for figures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#a magic function that allows inline plotting so figures are rendered in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#pandas for ...\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84c216",
   "metadata": {},
   "source": [
    "## Setup and Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from the cv file using pandas\n",
    "scores = pd.read_csv(\"Numeric Only Complete MCAT Data.csv\", header = 1)\n",
    "\n",
    "# rename some columns for consistency\n",
    "scores.rename(columns = {'Days Before Exam':'Days Before Exam.1', 'Days Before Exam.1':'Days Before Exam.2', 'Days Before Exam.2':'Days Before Exam.3', 'Days Before Exam.3':'Days Before Exam.4'}, inplace = True)\n",
    "\n",
    "# preview first five lines\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ddb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "scores.drop(columns=['C/P Score', 'CARS Score', 'B/B Score', 'P/S Score', 'Total Score', 'Total Score.1', 'Total Score.2', 'Total Score.3'], inplace = True)\n",
    "\n",
    "# sanity check: take a look at the data\n",
    "print('Scores data shape:', scores.shape)\n",
    "\n",
    "# look at distribution of scores\n",
    "scores['Real Score'].value_counts()\n",
    "\n",
    "# gives count, mean, standard deviation, min, max, and percentiles (including median)\n",
    "scores.describe(percentiles = [0.25, 0.5, 0.75], include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dd02f-b89c-40a8-a020-b45bf8724052",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fec19-8bab-4002-b691-91ca78057c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with impossible values\n",
    "#                # days before exam > 180\n",
    "#                # exact duplicate values to another row\n",
    "#                # impossible scores (>528 total or >132 on any subsection)\n",
    "remove_indices = []\n",
    "remove_indices.extend(scores[scores['Real Score']>528].index.values)\n",
    "remove_indices.extend(scores[scores['Real Score']<472].index.values)\n",
    "for i in range(1, 5):\n",
    "    remove_indices.extend(scores[scores['Days Before Exam.'+str(i)]>200].index.values)\n",
    "    remove_indices.extend(scores[scores['Days Before Exam.'+str(i)]<0].index.values)\n",
    "    remove_indices.extend(scores[scores['C/P Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['C/P Score.'+str(i)]<118].index.values)\n",
    "    remove_indices.extend(scores[scores['CARS Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['CARS Score.'+str(i)]<118].index.values)\n",
    "    remove_indices.extend(scores[scores['B/B Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['B/B Score.'+str(i)]<118].index.values)\n",
    "    remove_indices.extend(scores[scores['P/S Score.'+str(i)]>132].index.values)\n",
    "    remove_indices.extend(scores[scores['P/S Score.'+str(i)]<118].index.values)\n",
    "\n",
    "# remove duplicate indices\n",
    "remove_indices = list(dict.fromkeys(remove_indices))\n",
    "\n",
    "# delete all rows with impossible values\n",
    "for i in remove_indices:\n",
    "    scores.drop(i, axis = 0, inplace = True)\n",
    "\n",
    "scores.info()\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540a11c",
   "metadata": {},
   "source": [
    "## Visualize and Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install missingno\n",
    "import missingno as msno\n",
    "\n",
    "# visualize missing data with Missingno\n",
    "msno.bar(scores)\n",
    "msno.matrix(scores)\n",
    "\n",
    "# see if there is a reason for missing data\n",
    "msno.heatmap(scores)\n",
    "msno.dendrogram(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10158e-8976-474c-a695-f9744c1d6ac9",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Init the transformer\n",
    "knn_imp = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Fit/transform\n",
    "scores.loc[:, :] = knn_imp.fit_transform(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of columns, column labels, column data types, memory usage, range index, and non-null number of cells in each column\n",
    "scores.info()\n",
    "\n",
    "# preview data\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and labels from the observations\n",
    "features = scores.columns[1:len(scores)]\n",
    "X = scores[features].values.reshape(-1, len(features))\n",
    "y = scores[\"Real Score\"].to_numpy()\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f9ee9-20bc-4914-8f0b-97f6ca39322c",
   "metadata": {},
   "source": [
    "# Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df95f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "#split data into training and testing, fix random_state so output is the same every run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True,\n",
    "                                   test_size=0.20, random_state=5)\n",
    "\n",
    "#We define a function to see if our two sets are distributed evenly for every features\n",
    "def check_distributions(X1,X2):\n",
    "    pVals = np.zeros((X1.shape[1]))\n",
    "    for i in np.arange(X1.shape[1]):\n",
    "        kstat,pvalue = ks_2samp(X1[:,i],X2[:,i])\n",
    "        pVals[i]=pvalue\n",
    "    print(\"Mean of pvalues :\", np.mean(pVals))\n",
    "    print(\"Smallest pvalue :\",np.amin(pVals)) \n",
    "    #return(pVals)\n",
    "\n",
    "#sanity check: look at how many data points are in train/test and if our pvalues are acceptable\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])\n",
    "check_distributions(X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab62ad",
   "metadata": {},
   "source": [
    "## Visualize Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix with pandas\n",
    "corr_matrix = scores.corr()\n",
    "corr_matrix['Real Score'].sort_values(ascending=False)\n",
    "\n",
    "# create scatter matrices displaying totals, medians.\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = ['Real Score', 'CARS Score.1', 'CARS Score.2', 'CARS Score.3', 'CARS Score.4']\n",
    "scatter_matrix(scores[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a1508",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "## Define Model Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(X, y, results_list, scores_list):\n",
    "    lin_reg = LinearRegression()\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    results_dictionary = cross_validate(lin_reg, X, y, scoring=scoring, cv=4, return_estimator=True)\n",
    "    results_list.append(results_dictionary)\n",
    "    mean_score = results_dictionary[\"test_score\"].mean()\n",
    "    scores_list.append(mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab5ea9",
   "metadata": {},
   "source": [
    "## PCA to Create Linear Combinations of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eb1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# fit the PCA\n",
    "N = 12\n",
    "pca = PCA(n_components=N)\n",
    "pca.fit_transform(X_train)\n",
    "\n",
    "# plot the explained variances\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "color = 'tab:blue'\n",
    "ax1.bar(1+np.arange(N), pca.explained_variance_ratio_, color=color)\n",
    "ax1.set_xticks(1+np.arange(N, step=2))\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel(\"Explained variance ratio\", color=color)\n",
    "ax1.set_xlabel(\"Generated feature\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.plot(1+np.arange(N), np.cumsum(pca.explained_variance_ratio_), color=color)\n",
    "ax2.set_ylabel(\"Cumulative explained variance ratio\", color=color)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6e5d2-bff2-4999-a43b-311f484d08f5",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e82372-6a0f-4cf3-ad8d-8f48359a78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results stores the dictionary returned by the cross validation\n",
    "# results thus becomes a list of dictionaries\n",
    "# nmse stores the *mean* of all the neg_mean_squared_error's from each of k runs\n",
    "results = []\n",
    "nmse = []\n",
    "\n",
    "# Linear Regression without Dimensionality Reduction\n",
    "model_validation(X_train, y_train, results, nmse)\n",
    "# Linear Regression with Dimensionality Reduction\n",
    "for n in range(X_train.shape[1]):\n",
    "    pca = PCA(n_components=n+1)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "    model_validation(X_train_reduced, y_train, results, nmse)\n",
    "# Display the results\n",
    "for i in range(0, len(nmse), 1):\n",
    "    if i == 0:\n",
    "        print(\"neg mean squared error of linear regression: \", nmse[i])\n",
    "    else:\n",
    "        print(\"neg mean squared error of linear regression with \", i, \" dimensions is \", nmse[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087bb9-5622-4311-9e37-689a401bc5b9",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f435298-3728-470f-97ea-b57e3485a112",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d00ff-92f2-4c7e-a624-4af8d63a830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Train the model on training data and validate data using k-fold cross validation\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "rf_results_dictionary = cross_validate(rf, X_train, y_train, scoring=scoring, cv=4, return_estimator=True)\n",
    "rf_mean_score = rf_results_dictionary[\"test_score\"].mean()\n",
    "print(rf_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b33e62-d803-4d63-8fb7-c2214b2bbc6b",
   "metadata": {},
   "source": [
    "# Evaluate the Metrics of the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921d950-8643-4ddc-9920-f4788322edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The choosed model is PCA with 10 components\n",
    "pca = PCA(n_components=10)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# train the model on all the training data\n",
    "lin_reg1 = LinearRegression()\n",
    "lin_reg1 = lin_reg1.fit(X_train_reduced, y_train)\n",
    "\n",
    "# transform the testing data to the same space as the training data\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# evaluate the model's accuracy using the test data\n",
    "R2 = lin_reg1.score(X_test_reduced, y_test)\n",
    "print(f\"The coefficient of determination: {R2:.2f}\")\n",
    "\n",
    "# use loss function on prediction vs. test\n",
    "y_pred = lin_reg1.predict(X_test_reduced)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"neg mean squared error of the final ML model: \", -mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d689f86-f4e7-4214-b597-d5fb16894714",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_rf)\n",
    "print(\"neg mean squared error of the final ML model: \", -mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4c4bd",
   "metadata": {},
   "source": [
    "# Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display what pca 1 is\n",
    "plt.figure()\n",
    "\n",
    "print(X.shape)\n",
    "D,E = np.linalg.eig(np.matmul(X.T,X))\n",
    "#print(D)\n",
    "#print(D.shape)\n",
    "#E = E.reshape()\n",
    "z = list(scores.columns[1:])#np.arange(1,21)\n",
    "#print(z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (8,4))\n",
    "# We see that the highest eigenvalue is D[7] so row 7 of E is \n",
    "ax.bar(z, E[:,7])#, label=bar_labels, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel('PCA 1 dimension')\n",
    "ax.set_title('PCA 1 in fonction of the features')\n",
    "for label in ax.get_xticklabels(which='major'):\n",
    "    label.set(rotation=30, horizontalalignment='right',fontsize=8)\n",
    "#ax.legend(title='Fruit color')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.plot(z,E[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda20525",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "# convert the labels to numbers, each will be assigned a separate color based on the cmap specified\n",
    "colors = [int(x) for x in y_train]\n",
    "sc = plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], c=colors)#,s=1 cmap='tab10')\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend(*sc.legend_elements(), title='digit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b76ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plotting\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], X_train_reduced[:,2], c=colors)\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "plt.legend(*sc.legend_elements(), title='digit',loc=\"upper center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plotting\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], y_train)#, c=colors)\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel(\"Final score\")\n",
    "#plt.legend(*sc.legend_elements(), title='digit',loc=\"upper center\")\n",
    "plt.title(\"Final scores at exam of the training set in function of PCA1 and PCA2 parameters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "# convert the labels to numbers, each will be assigned a separate color based on the cmap specified\n",
    "colors = [int(x) for x in y_train]\n",
    "sc = plt.scatter(X_train_reduced[:, 0],y_train)#, c=colors)#,s=1 cmap='tab10')\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"Final score\")\n",
    "#plt.legend(*sc.legend_elements(), title='digit')\n",
    "plt.title(\"Final scores at exam of the training set in function of PCA1 parameter\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a85bc-09ac-4540-a310-f7aee2ed6219",
   "metadata": {},
   "source": [
    "## Visualize RF Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a94dbc-ff9c-47f6-b8d1-db4b63efe460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plot_tree(rf.estimators_[0], \n",
    "          feature_names = features,\n",
    "          filled = True, impurity = True, \n",
    "          rounded = True)\n",
    "\n",
    "#from sklearn.tree import export_text\n",
    "\n",
    "#print(export_text(rf.estimators_[0], \n",
    "                  #spacing=3, decimals=3,\n",
    "                  #feature_names=features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a3e73-e778-4ccc-bde8-8464665b9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda uninstall python-graphviz\n",
    "#conda uninstall graphviz\n",
    "#pip install dtreeviz             # install dtreeviz for sklearn\n",
    "#pip install dtreeviz[xgboost]    # install XGBoost related dependency\n",
    "#pip install dtreeviz[pyspark]    # install pyspark related dependency\n",
    "#pip install dtreeviz[lightgbm]   # install LightGBM related dependency\n",
    "#pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbe059-df11-40c0-9f3d-a1864e1e5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import dtreeviz\n",
    "\n",
    "viz = dtreeviz(rf.estimators_[0], X_train, y_train,\n",
    "               target_name = \"Real MCAT Score\",\n",
    "               feature_names = features,\n",
    "               title = \"Decision Tree for MCAT Scores\")\n",
    "\n",
    "viz.save(\"MCAT_decision_tree_with_imputation.svg\")            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
